{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLamaIndex + GitHub Models for RAG\n",
    "\n",
    "This notebook demonstrates how to perform Retrieval-Augmented Generation (RAG) with LLamaIndex.\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Retrieval-Augmented Generation (RAG) is a technique in natural language processing that combines the strengths of retrieval-based and generation-based methods to enhance the quality and accuracy of generated text. It integrates a retriever module, which searches a large corpus of documents for relevant information, with a generator module to produce coherent and contextually appropriate responses. This hybrid approach allows RAG to leverage vast amounts of external knowledge stored in documents, making it particularly effective for tasks requiring detailed information and context beyond the model's pre-existing knowledge.\n",
    "\n",
    "RAG operates by first using the retriever to identify the most relevant pieces of information from a database or collection of texts. These retrieved passages are then fed into the generator, which synthesizes the information to produce a final response. This process enables the model to provide more accurate and informative answers, as it dynamically incorporates up-to-date and specific details from the retrieval stage. The combination of retrieval and generation ensures that RAG models are both knowledgeable and flexible, making them valuable for applications such as question answering, summarization, and dialogue systems.\n",
    "\n",
    "In this sample, we will create an index from a set of markdown documents that contain product descriptions. Using a retriever, we will search the index with a user question to find the most relevant documents. Then we will use llama-index's query for a full Retrieval-Augmented Generation (RAG) implementation.\n",
    "\n",
    "## 1. Install dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: llama-index in /workspaces/codespaces-models/.venv/lib/python3.11/site-packages (0.11.9)\n",
      "Requirement already satisfied: llama-index-agent-openai<0.4.0,>=0.3.1 in /workspaces/codespaces-models/.venv/lib/python3.11/site-packages (from llama-index) (0.3.1)\n",
      "Requirement already satisfied: llama-index-cli<0.4.0,>=0.3.1 in /workspaces/codespaces-models/.venv/lib/python3.11/site-packages (from llama-index) (0.3.1)\n",
      "Requirement already satisfied: llama-index-core<0.12.0,>=0.11.9 in /workspaces/codespaces-models/.venv/lib/python3.11/site-packages (from llama-index) (0.11.9)\n",
      "Requirement already satisfied: llama-index-embeddings-openai<0.3.0,>=0.2.4 in /workspaces/codespaces-models/.venv/lib/python3.11/site-packages (from llama-index) (0.2.5)\n",
      "Requirement already satisfied: llama-index-indices-managed-llama-cloud>=0.3.0 in /workspaces/codespaces-models/.venv/lib/python3.11/site-packages (from llama-index) (0.3.1)\n",
      "Requirement already satisfied: llama-index-legacy<0.10.0,>=0.9.48 in /workspaces/codespaces-models/.venv/lib/python3.11/site-packages (from llama-index) (0.9.48.post3)\n",
      "Requirement already satisfied: llama-index-llms-openai<0.3.0,>=0.2.3 in /workspaces/codespaces-models/.venv/lib/python3.11/site-packages (from llama-index) (0.2.7)\n",
      "Requirement already satisfied: llama-index-multi-modal-llms-openai<0.3.0,>=0.2.0 in /workspaces/codespaces-models/.venv/lib/python3.11/site-packages (from llama-index) (0.2.1)\n",
      "Requirement already satisfied: llama-index-program-openai<0.3.0,>=0.2.0 in /workspaces/codespaces-models/.venv/lib/python3.11/site-packages (from llama-index) (0.2.0)\n",
      "Requirement already satisfied: llama-index-question-gen-openai<0.3.0,>=0.2.0 in /workspaces/codespaces-models/.venv/lib/python3.11/site-packages (from llama-index) (0.2.0)\n",
      "Requirement already satisfied: llama-index-readers-file<0.3.0,>=0.2.0 in /workspaces/codespaces-models/.venv/lib/python3.11/site-packages (from llama-index) (0.2.1)\n",
      "Requirement already satisfied: llama-index-readers-llama-parse>=0.3.0 in /workspaces/codespaces-models/.venv/lib/python3.11/site-packages (from llama-index) (0.3.0)\n",
      "Requirement already satisfied: nltk>3.8.1 in /workspaces/codespaces-models/.venv/lib/python3.11/site-packages (from llama-index) (3.9.1)\n",
      "Requirement already satisfied: openai>=1.14.0 in /workspaces/codespaces-models/.venv/lib/python3.11/site-packages (from llama-index-agent-openai<0.4.0,>=0.3.1->llama-index) (1.45.1)\n",
      "Requirement already satisfied: PyYAML>=6.0.1 in /workspaces/codespaces-models/.venv/lib/python3.11/site-packages (from llama-index-core<0.12.0,>=0.11.9->llama-index) (6.0.2)\n",
      "Requirement already satisfied: SQLAlchemy>=1.4.49 in /workspaces/codespaces-models/.venv/lib/python3.11/site-packages (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.12.0,>=0.11.9->llama-index) (2.0.34)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.6 in /workspaces/codespaces-models/.venv/lib/python3.11/site-packages (from llama-index-core<0.12.0,>=0.11.9->llama-index) (3.10.5)\n",
      "Requirement already satisfied: dataclasses-json in /workspaces/codespaces-models/.venv/lib/python3.11/site-packages (from llama-index-core<0.12.0,>=0.11.9->llama-index) (0.6.7)\n",
      "Requirement already satisfied: deprecated>=1.2.9.3 in /workspaces/codespaces-models/.venv/lib/python3.11/site-packages (from llama-index-core<0.12.0,>=0.11.9->llama-index) (1.2.14)\n",
      "Requirement already satisfied: dirtyjson<2.0.0,>=1.0.8 in /workspaces/codespaces-models/.venv/lib/python3.11/site-packages (from llama-index-core<0.12.0,>=0.11.9->llama-index) (1.0.8)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /workspaces/codespaces-models/.venv/lib/python3.11/site-packages (from llama-index-core<0.12.0,>=0.11.9->llama-index) (2024.9.0)\n",
      "Requirement already satisfied: httpx in /workspaces/codespaces-models/.venv/lib/python3.11/site-packages (from llama-index-core<0.12.0,>=0.11.9->llama-index) (0.27.2)\n",
      "Requirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in /workspaces/codespaces-models/.venv/lib/python3.11/site-packages (from llama-index-core<0.12.0,>=0.11.9->llama-index) (1.6.0)\n",
      "Requirement already satisfied: networkx>=3.0 in /workspaces/codespaces-models/.venv/lib/python3.11/site-packages (from llama-index-core<0.12.0,>=0.11.9->llama-index) (3.3)\n",
      "Requirement already satisfied: numpy<2.0.0 in /workspaces/codespaces-models/.venv/lib/python3.11/site-packages (from llama-index-core<0.12.0,>=0.11.9->llama-index) (1.26.4)\n",
      "Requirement already satisfied: pillow>=9.0.0 in /workspaces/codespaces-models/.venv/lib/python3.11/site-packages (from llama-index-core<0.12.0,>=0.11.9->llama-index) (10.4.0)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.0 in /workspaces/codespaces-models/.venv/lib/python3.11/site-packages (from llama-index-core<0.12.0,>=0.11.9->llama-index) (2.9.1)\n",
      "Requirement already satisfied: requests>=2.31.0 in /workspaces/codespaces-models/.venv/lib/python3.11/site-packages (from llama-index-core<0.12.0,>=0.11.9->llama-index) (2.32.3)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.2.0 in /workspaces/codespaces-models/.venv/lib/python3.11/site-packages (from llama-index-core<0.12.0,>=0.11.9->llama-index) (8.5.0)\n",
      "Requirement already satisfied: tiktoken>=0.3.3 in /workspaces/codespaces-models/.venv/lib/python3.11/site-packages (from llama-index-core<0.12.0,>=0.11.9->llama-index) (0.7.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.66.1 in /workspaces/codespaces-models/.venv/lib/python3.11/site-packages (from llama-index-core<0.12.0,>=0.11.9->llama-index) (4.66.5)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in /workspaces/codespaces-models/.venv/lib/python3.11/site-packages (from llama-index-core<0.12.0,>=0.11.9->llama-index) (4.12.2)\n",
      "Requirement already satisfied: typing-inspect>=0.8.0 in /workspaces/codespaces-models/.venv/lib/python3.11/site-packages (from llama-index-core<0.12.0,>=0.11.9->llama-index) (0.9.0)\n",
      "Requirement already satisfied: wrapt in /workspaces/codespaces-models/.venv/lib/python3.11/site-packages (from llama-index-core<0.12.0,>=0.11.9->llama-index) (1.16.0)\n",
      "Requirement already satisfied: llama-cloud>=0.0.11 in /workspaces/codespaces-models/.venv/lib/python3.11/site-packages (from llama-index-indices-managed-llama-cloud>=0.3.0->llama-index) (0.0.17)\n",
      "Requirement already satisfied: pandas in /workspaces/codespaces-models/.venv/lib/python3.11/site-packages (from llama-index-legacy<0.10.0,>=0.9.48->llama-index) (2.2.2)\n",
      "Requirement already satisfied: beautifulsoup4<5.0.0,>=4.12.3 in /workspaces/codespaces-models/.venv/lib/python3.11/site-packages (from llama-index-readers-file<0.3.0,>=0.2.0->llama-index) (4.12.3)\n",
      "Requirement already satisfied: pypdf<5.0.0,>=4.0.1 in /workspaces/codespaces-models/.venv/lib/python3.11/site-packages (from llama-index-readers-file<0.3.0,>=0.2.0->llama-index) (4.3.1)\n",
      "Requirement already satisfied: striprtf<0.0.27,>=0.0.26 in /workspaces/codespaces-models/.venv/lib/python3.11/site-packages (from llama-index-readers-file<0.3.0,>=0.2.0->llama-index) (0.0.26)\n",
      "Requirement already satisfied: llama-parse>=0.5.0 in /workspaces/codespaces-models/.venv/lib/python3.11/site-packages (from llama-index-readers-llama-parse>=0.3.0->llama-index) (0.5.5)\n",
      "Requirement already satisfied: click in /workspaces/codespaces-models/.venv/lib/python3.11/site-packages (from nltk>3.8.1->llama-index) (8.1.7)\n",
      "Requirement already satisfied: joblib in /workspaces/codespaces-models/.venv/lib/python3.11/site-packages (from nltk>3.8.1->llama-index) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /workspaces/codespaces-models/.venv/lib/python3.11/site-packages (from nltk>3.8.1->llama-index) (2024.9.11)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /workspaces/codespaces-models/.venv/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.9->llama-index) (2.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /workspaces/codespaces-models/.venv/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.9->llama-index) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /workspaces/codespaces-models/.venv/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.9->llama-index) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /workspaces/codespaces-models/.venv/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.9->llama-index) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /workspaces/codespaces-models/.venv/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.9->llama-index) (6.1.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /workspaces/codespaces-models/.venv/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.9->llama-index) (1.11.1)\n",
      "Requirement already satisfied: soupsieve>1.2 in /workspaces/codespaces-models/.venv/lib/python3.11/site-packages (from beautifulsoup4<5.0.0,>=4.12.3->llama-index-readers-file<0.3.0,>=0.2.0->llama-index) (2.6)\n",
      "Requirement already satisfied: anyio in /workspaces/codespaces-models/.venv/lib/python3.11/site-packages (from httpx->llama-index-core<0.12.0,>=0.11.9->llama-index) (4.4.0)\n",
      "Requirement already satisfied: certifi in /workspaces/codespaces-models/.venv/lib/python3.11/site-packages (from httpx->llama-index-core<0.12.0,>=0.11.9->llama-index) (2024.8.30)\n",
      "Requirement already satisfied: httpcore==1.* in /workspaces/codespaces-models/.venv/lib/python3.11/site-packages (from httpx->llama-index-core<0.12.0,>=0.11.9->llama-index) (1.0.5)\n",
      "Requirement already satisfied: idna in /workspaces/codespaces-models/.venv/lib/python3.11/site-packages (from httpx->llama-index-core<0.12.0,>=0.11.9->llama-index) (3.8)\n",
      "Requirement already satisfied: sniffio in /workspaces/codespaces-models/.venv/lib/python3.11/site-packages (from httpx->llama-index-core<0.12.0,>=0.11.9->llama-index) (1.3.1)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /workspaces/codespaces-models/.venv/lib/python3.11/site-packages (from httpcore==1.*->httpx->llama-index-core<0.12.0,>=0.11.9->llama-index) (0.14.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /workspaces/codespaces-models/.venv/lib/python3.11/site-packages (from openai>=1.14.0->llama-index-agent-openai<0.4.0,>=0.3.1->llama-index) (1.9.0)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /workspaces/codespaces-models/.venv/lib/python3.11/site-packages (from openai>=1.14.0->llama-index-agent-openai<0.4.0,>=0.3.1->llama-index) (0.5.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /workspaces/codespaces-models/.venv/lib/python3.11/site-packages (from pydantic<3.0.0,>=2.7.0->llama-index-core<0.12.0,>=0.11.9->llama-index) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.3 in /workspaces/codespaces-models/.venv/lib/python3.11/site-packages (from pydantic<3.0.0,>=2.7.0->llama-index-core<0.12.0,>=0.11.9->llama-index) (2.23.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /workspaces/codespaces-models/.venv/lib/python3.11/site-packages (from requests>=2.31.0->llama-index-core<0.12.0,>=0.11.9->llama-index) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /workspaces/codespaces-models/.venv/lib/python3.11/site-packages (from requests>=2.31.0->llama-index-core<0.12.0,>=0.11.9->llama-index) (2.2.2)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /workspaces/codespaces-models/.venv/lib/python3.11/site-packages (from SQLAlchemy>=1.4.49->SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.12.0,>=0.11.9->llama-index) (3.1.0)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /workspaces/codespaces-models/.venv/lib/python3.11/site-packages (from typing-inspect>=0.8.0->llama-index-core<0.12.0,>=0.11.9->llama-index) (1.0.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /workspaces/codespaces-models/.venv/lib/python3.11/site-packages (from dataclasses-json->llama-index-core<0.12.0,>=0.11.9->llama-index) (3.22.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /workspaces/codespaces-models/.venv/lib/python3.11/site-packages (from pandas->llama-index-legacy<0.10.0,>=0.9.48->llama-index) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /workspaces/codespaces-models/.venv/lib/python3.11/site-packages (from pandas->llama-index-legacy<0.10.0,>=0.9.48->llama-index) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /workspaces/codespaces-models/.venv/lib/python3.11/site-packages (from pandas->llama-index-legacy<0.10.0,>=0.9.48->llama-index) (2024.1)\n",
      "Requirement already satisfied: packaging>=17.0 in /workspaces/codespaces-models/.venv/lib/python3.11/site-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json->llama-index-core<0.12.0,>=0.11.9->llama-index) (24.1)\n",
      "Requirement already satisfied: six>=1.5 in /workspaces/codespaces-models/.venv/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas->llama-index-legacy<0.10.0,>=0.9.48->llama-index) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: openai in /workspaces/codespaces-models/.venv/lib/python3.11/site-packages (1.45.1)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /workspaces/codespaces-models/.venv/lib/python3.11/site-packages (from openai) (4.4.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /workspaces/codespaces-models/.venv/lib/python3.11/site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /workspaces/codespaces-models/.venv/lib/python3.11/site-packages (from openai) (0.27.2)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /workspaces/codespaces-models/.venv/lib/python3.11/site-packages (from openai) (0.5.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /workspaces/codespaces-models/.venv/lib/python3.11/site-packages (from openai) (2.9.1)\n",
      "Requirement already satisfied: sniffio in /workspaces/codespaces-models/.venv/lib/python3.11/site-packages (from openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in /workspaces/codespaces-models/.venv/lib/python3.11/site-packages (from openai) (4.66.5)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in /workspaces/codespaces-models/.venv/lib/python3.11/site-packages (from openai) (4.12.2)\n",
      "Requirement already satisfied: idna>=2.8 in /workspaces/codespaces-models/.venv/lib/python3.11/site-packages (from anyio<5,>=3.5.0->openai) (3.8)\n",
      "Requirement already satisfied: certifi in /workspaces/codespaces-models/.venv/lib/python3.11/site-packages (from httpx<1,>=0.23.0->openai) (2024.8.30)\n",
      "Requirement already satisfied: httpcore==1.* in /workspaces/codespaces-models/.venv/lib/python3.11/site-packages (from httpx<1,>=0.23.0->openai) (1.0.5)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /workspaces/codespaces-models/.venv/lib/python3.11/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /workspaces/codespaces-models/.venv/lib/python3.11/site-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.3 in /workspaces/codespaces-models/.venv/lib/python3.11/site-packages (from pydantic<3,>=1.9.0->openai) (2.23.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: python-dotenv in /workspaces/codespaces-models/.venv/lib/python3.11/site-packages (1.0.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install llama-index\n",
    "%pip install openai\n",
    "%pip install python-dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Setup classes to a chat model and an embedding model\n",
    "\n",
    "To run RAG, you need 2 models: a chat model, and an embedding model. The GitHub Model service offers different options.\n",
    "\n",
    "For instance you could use an Azure OpenAI chat model (`gpt-4o-mini`) and embedding model (`text-embedding-3-small`), or a Cohere chat model (`Cohere-command-r-plus`) and embedding model (`Cohere-embed-v3-multilingual`).\n",
    "\n",
    "We'll proceed using some of the Azure OpenAI models below. You can find [how to leverage Cohere models in the LlamaIndex documentation](https://docs.llamaindex.ai/en/stable/examples/llm/cohere/).\n",
    "\n",
    "### Example using Azure OpenAI models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import dotenv\n",
    "\n",
    "dotenv.load_dotenv()\n",
    "\n",
    "if not os.getenv(\"GITHUB_TOKEN\"):\n",
    "    raise ValueError(\"GITHUB_TOKEN is not set\")\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"GITHUB_TOKEN\")\n",
    "os.environ[\"OPENAI_BASE_URL\"] = \"https://models.inference.ai.azure.com/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, we are setting up the embedding model and the llm model to be used. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader\n",
    "from llama_index.core import Settings\n",
    "import logging\n",
    "import sys, os\n",
    "\n",
    "logging.basicConfig(\n",
    "    stream=sys.stdout, level=logging.INFO\n",
    ")  # logging.DEBUG for more verbose output\n",
    "logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))\n",
    "\n",
    "llm = OpenAI(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    api_key=os.getenv(\"OPENAI_API_KEY\"),\n",
    "    api_base=os.getenv(\"OPENAI_BASE_URL\"),\n",
    ")\n",
    "\n",
    "embed_model = OpenAIEmbedding(\n",
    "    model=\"text-embedding-3-small\",\n",
    "    api_key=os.getenv(\"OPENAI_API_KEY\"),\n",
    "    api_base=os.getenv(\"OPENAI_BASE_URL\"),\n",
    ")\n",
    "\n",
    "Settings.llm = llm\n",
    "Settings.embed_model = embed_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Create an index and retriever\n",
    "\n",
    "In the data folder, we have some product information files in markdown format. Here is a sample of the data:\n",
    "\n",
    "```markdown\n",
    "# Information about product item_number: 1\n",
    "TrailMaster X4 Tent, price $250,\n",
    "\n",
    "\n",
    "## Brand\n",
    "OutdoorLiving\n",
    "\n",
    "Main Category: CAMPING & HIKING\n",
    "Sub Category: TENTS & SHELTERS\n",
    "Product Type: BACKPACKING TENTS\n",
    "\n",
    "## Features\n",
    "- Polyester material for durability\n",
    "- Spacious interior to accommodate multiple people\n",
    "- Easy setup with included instructions\n",
    "- Water-resistant construction to withstand light rain\n",
    "- Mesh panels for ventilation and insect protection\n",
    "- Rainfly included for added weather protection\n",
    "- Multiple doors for convenient entry and exit\n",
    "- Interior pockets for organizing small items\n",
    "- Reflective guy lines for improved visibility at night\n",
    "- Freestanding design for easy setup and relocation\n",
    "- Carry bag included for convenient storage and transportation\n",
    "```\n",
    "Here is the link to the full file: [data/product_info_1.md](data/product_info_1.md). As you can see, the files are rather long and contain different sections like Brand, Features, User Guide, Warranty Information, Reviews, etc. All these can be useful when answering user questions.\n",
    "\n",
    "To be able to find the right information, we will create a vector index that stores the embeddings of the documents. Note that we are reducing the batch size of the indexer to prevent rate limiting. The GitHub Model Service is rate limited to 64K tokens per request for embedding models.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://models.inference.ai.azure.com/embeddings \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://models.inference.ai.azure.com/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://models.inference.ai.azure.com/embeddings \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://models.inference.ai.azure.com/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://models.inference.ai.azure.com/embeddings \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://models.inference.ai.azure.com/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://models.inference.ai.azure.com/embeddings \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://models.inference.ai.azure.com/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://models.inference.ai.azure.com/embeddings \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://models.inference.ai.azure.com/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "source": [
    "#Note: we have to reduce the batch size to stay within the token limits of the free service\n",
    "documents = SimpleDirectoryReader(\"data\").load_data()\n",
    "index = VectorStoreIndex.from_documents(documents, insert_batch_size=150)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have an index, we can use the retriever to find the most relevant documents for a user question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://models.inference.ai.azure.com/embeddings \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://models.inference.ai.azure.com/embeddings \"HTTP/1.1 200 OK\"\n",
      "Node ID: 9a50a4cf-f48d-43c8-8ce3-263e434e8ae1\n",
      "Text: 3. Temperature Rating and Comfort: The MountainDream Sleeping\n",
      "Bag is rated for temperatures between 15°F to 30°F. However, personal\n",
      "comfort preferences may vary. It is recommended to use additional\n",
      "layers or adjust ventilation using the zipper and hood to achieve the\n",
      "desired temperature.\n",
      "Score:  0.623\n",
      "\n",
      "Node ID: 1a907901-927d-4054-a1b0-6e13ed1db975\n",
      "Text: FAQ 31) What is the temperature rating of the CozyNights\n",
      "Sleeping Bag?    The CozyNights Sleeping Bag is rated for 3-season use\n",
      "and has a temperature rating of 20�F to 60�F (-6�C to 15�C).  32) Can\n",
      "the CozyNights Sleeping Bag be zipped together with another sleeping\n",
      "bag to create a double sleeping bag?    Yes, two CozyNights Sleeping\n",
      "Bags can be...\n",
      "Score:  0.617\n",
      "\n"
     ]
    }
   ],
   "source": [
    "retriever = index.as_retriever()\n",
    "#fragments = retriever.retrieve(\"What is the temperature rating of the cozynights sleeping bag?\")\n",
    "fragments = retriever.retrieve(\"What is the temperature rating of the Folding Table sleeping bag?\")\n",
    "\n",
    "for fragment in fragments:\n",
    "    print(fragment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Use chat model to generate an answer\n",
    "\n",
    "Now that we have the documents that match the user question, we can ask our chat model to generate an answer based on the retrieved documents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://models.inference.ai.azure.com/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://models.inference.ai.azure.com/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\n",
      "assistant: The context data does not provide information about the temperature rating of the Folding Table sleeping bag. You may need to check the manufacturer's specifications or product details for that information.\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core.llms import ChatMessage\n",
    "\n",
    "context = \"\\n------\\n\".join([ fragment.text for fragment in fragments ])\n",
    "\n",
    "messages = [\n",
    "    ChatMessage(role=\"system\", content=\"You are a helpful assistant that answers some questions with the help of some context data.\\n\\nHere is the context data:\\n\\n\" + context),\n",
    "    ChatMessage(role=\"user\", content=\"What is the temperature rating of the Folding Table sleeping bag?\")\n",
    "    #ChatMessage(role=\"user\", content=\"What is the temperature rating of the cozynights sleeping bag?\")\n",
    "]\n",
    "\n",
    "response = llm.chat(messages)\n",
    "print()\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LLamaIndex provides a simple API to query the retriever and the generator in one go. The query function takes the question as input and returns the answer generated by the generator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://models.inference.ai.azure.com/embeddings \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://models.inference.ai.azure.com/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://models.inference.ai.azure.com/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://models.inference.ai.azure.com/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\n",
      "The CozyNights Sleeping Bag is rated for 3-season use with a temperature range of 20°F to 60°F (-6°C to 15°C).\n"
     ]
    }
   ],
   "source": [
    "query_engine = index.as_query_engine()\n",
    "response = query_engine.query(\"What is the temperature rating of the cozynights sleeping bag?\")\n",
    "print()\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://models.inference.ai.azure.com/embeddings \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://models.inference.ai.azure.com/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://models.inference.ai.azure.com/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://models.inference.ai.azure.com/chat/completions \"HTTP/1.1 200 OK\"\n",
      "A good option for a 2-person tent is the SkyView 2-Person Tent. It is praised for its spaciousness, ease of setup, and durability. Users have highlighted its excellent ventilation, waterproof design, and thoughtful storage features, making it suitable for various camping conditions.\n"
     ]
    }
   ],
   "source": [
    "response = query_engine.query(\"What is a good 2 person tent?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://models.inference.ai.azure.com/embeddings \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://models.inference.ai.azure.com/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://models.inference.ai.azure.com/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://models.inference.ai.azure.com/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Yes, the SkyView 2-Person Tent includes a rainfly as one of its components.\n"
     ]
    }
   ],
   "source": [
    "response = query_engine.query(\"Does the SkyView 2-Person Tent have a rain fly?\")\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gh-cookbook",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
